{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Convexity and Regularization\n",
    "\n",
    "CPSC 340: Machine Learning and Data Mining\n",
    "\n",
    "The University of British Columbia\n",
    "\n",
    "Adapted from Mike Gelbart's notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "from sklearn.linear_model import LinearRegression, HuberRegressor, Ridge, Lasso\n",
    "from scipy.optimize import minimize\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rcParams['font.size'] = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topics:\n",
    "\n",
    "- Convexity\n",
    "- Why L1 leads to sparsity\n",
    "- Regularization strength intuition, regularization paths, L1 vs. L2\n",
    "- Why not use Huber regularization?\n",
    "- Uniqueness of solution: L1 and L2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convexity\n",
    "\n",
    "Convex functions have many nice properties. For our purposes, the main one we care about is that all stationary points are global minima, i.e. $\\nabla f(w) = 0$ implies $w$ is a global minimizer. \n",
    "\n",
    "How do we show that a function is convex? Firstly, its domain must be a convex set (which is not of concern to us in 340 since we are typically doing unconstrained optimization). Past that, there are three main approaches:\n",
    "\n",
    "- Using the definition of convexity: $$f(\\theta w + (1-\\theta)v) \\leq \\theta f(w) + (1-\\theta) f(v), \\quad\\forall w, v \\in \\mathbb{R}, \\quad\\forall \\theta \\in [0,1]$$This always applies.\n",
    "- If $f$ is twice differentiable everywhere:\n",
    "    - $f''(w) \\geq 0$ for all $w$ (1D case)\n",
    "    - $\\nabla^2f(w) \\succeq 0$ for all $w$. In words: all eigenvalue of the Hessian matrix are nonnegative. \n",
    "- Operations that preserve convexity\n",
    "\n",
    "Commonly used operations that preserve convexity:\n",
    "- Any p-norm is convex (for p >= 1)\n",
    "- If $f$ and $g$ are convex functions:\n",
    "    - $h(w) = \\max(f(w), g(w))$ is convex\n",
    "    - $h(w) = f(Aw+b)$ is convex for any $A, b$\n",
    "    - $h(w) = \\alpha f(w)$ is convex for $\\alpha \\geq 0$\n",
    "    - $h(w) = f(w) + g(w)$ is convex \n",
    "- Note: f(g(w)) is NOT necessarily convex\n",
    "- See lecture slides \n",
    "\n",
    "#### Examples\n",
    "Show the following functions are convex\n",
    "- $f(w) = -\\log(w^2)$ on $0 < w < \\infty$\n",
    "- $f(w) = \\frac12 \\|Xw-y\\|^2+\\frac\\lambda2\\|w\\|^2$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why L1 regularization leads to sparsity\n",
    "\n",
    "Consider this 1D case of a feature whose \"true\" weight is close to zero. Starting with no regularization:\n",
    "\n",
    "$$f(w)=\\sum_{i=1}^n (wx_i-y_i)^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npr.seed(0)\n",
    "\n",
    "# Generate random 1D data set\n",
    "d = 1\n",
    "n = 30\n",
    "X = npr.randn(n,d)\n",
    "y = npr.randn(n,1)\n",
    "def f(w, X, y):\n",
    "    return np.sum((X@w-y)**2,axis=0)\n",
    "\n",
    "wgrid = np.arange(-1,1,2**(-5))\n",
    "fw = f(wgrid[None], X, y)\n",
    "minind = np.argmin(fw);\n",
    "minw = wgrid[minind]\n",
    "plt.plot(wgrid, fw);\n",
    "plt.xlabel(\"$w$\");\n",
    "plt.ylabel(\"$f(w)$\");\n",
    "plt.plot((minw, minw),plt.ylim(),'--k');\n",
    "plt.title(\"No regularization\"); # $f(w)=\\sum_{i=1}^n (wx_i-y_i)^2$\n",
    "print(\"minimizer: w=%.2f\" % minw);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before proceeding, ask yourself: do you understand what this picture is showing? It's the parameter space, but in 1D, so we don't need a contour plot like we did in 2D."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### L0 regularization\n",
    "\n",
    "$$f(w)=\\sum_{i=1}^n (wx_i-y_i)^2 + \\lambda \\|w\\|_0$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f0(w, X, y, λ=10):\n",
    "    return np.sum((X@w-y)**2,axis=0) + λ*(w!=0)\n",
    "fw0 = f0(wgrid[None], X, y).ravel()\n",
    "minind = np.argmin(fw0);\n",
    "minw = wgrid[minind]\n",
    "plt.plot(wgrid, fw0, '.',markersize=7);\n",
    "# plt.plot((minw, minw),plt.ylim(),'--k');\n",
    "# plt.plot(minw, fw0[minind], 'r*',markersize=10);\n",
    "plt.xlabel(\"$w$\");\n",
    "plt.ylabel(\"$f(w)$\");\n",
    "plt.title(\"L0 regularization\"); # $f(w)=\\sum_{i=1}^n (wx_i-y_i)^2$\n",
    "print(\"minimizer: w=%.2f\" % minw);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- L0-regularized minimum is often exactly at the ‘discontinuity’ at 0:\n",
    "  - Sets the feature to exactly 0 (does feature selection), but is **non-convex**.\n",
    "  \n",
    "- We can fiddle with the code to see a case where we don't get $w=0$:\n",
    "  - By making the un-regularized solution further from 0.\n",
    "  - By decreasing $\\lambda$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### L2 regularization\n",
    "\n",
    "$$f(w)=\\sum_{i=1}^n (wx_i-y_i)^2 + \\lambda \\|w\\|^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f2(w, X, y, λ=100):\n",
    "    return np.sum((X@w-y)**2,axis=0) + λ*np.sum(w**2,axis=0)\n",
    "fw2 = f2(wgrid[None], X, y).ravel()\n",
    "minind = np.argmin(fw2);\n",
    "minw = wgrid[minind]\n",
    "plt.plot(wgrid, fw2);\n",
    "plt.plot((minw, minw),plt.ylim(),'--k');\n",
    "# plt.plot(minw, fw0[minind], 'r*',markersize=10);\n",
    "plt.xlabel(\"$w$\");\n",
    "plt.ylabel(\"$f(w)$\");\n",
    "plt.title(\"L2 regularization\"); # $f(w)=\\sum_{i=1}^n (wx_i-y_i)^2$\n",
    "print(\"minimizer: w=%.2f\" % minw);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note the solution is closer to zero than without regularization, but still not zero.\n",
    "- Why? Because no incentive to move all the way to zero; the slope of the regularization term goes to zero!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### L1 regularization\n",
    "\n",
    "$$f(w)=\\sum_{i=1}^n (wx_i-y_i)^2 + \\lambda \\|w\\|_1$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "λ = 100\n",
    "def f1(w, X, y, λ=100):\n",
    "    return np.sum((X@w-y)**2,axis=0) + λ*np.sum(np.abs(w),axis=0)\n",
    "fw1 = f1(wgrid[None], X, y).ravel()\n",
    "minind = np.argmin(fw1);\n",
    "minw = wgrid[minind]\n",
    "plt.plot(wgrid, fw1);\n",
    "plt.plot((minw, minw),plt.ylim(),'--k');\n",
    "# plt.plot(minw, fw0[minind], 'r*',markersize=10);\n",
    "plt.xlabel(\"$w$\");\n",
    "plt.ylabel(\"$f(w)$\");\n",
    "plt.title(\"L1 regularization\"); # $f(w)=\\sum_{i=1}^n (wx_i-y_i)^2$\n",
    "print(\"minimizer: w=%.2f\" % minw);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Notice the solution is _exactly_ zero.\n",
    "- Again, whether this happens depends on $\\lambda$ and the original solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization strength intuition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: what is the effect of changing the regularization strength $\\lambda$?\n",
    "\n",
    "Let's take the oversimplified case of $n=1,d=1$ with no intercept and just one point at $(x_i,y_i)=(1,1)$. We can get zero training error by setting $w=1$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(1,1,'.',markersize=20)\n",
    "plt.plot((-3,3),(-3,3));\n",
    "plt.xlabel(\"x\");\n",
    "plt.ylabel(\"y\");\n",
    "plt.title(\"Extreme toy example: $d=1,n=1$\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our regularized loss is \n",
    "\n",
    "$$f(w)=(w-1)^2+\\lambda w^2$$\n",
    "\n",
    "- The left-hand term (data term) wants $w=1$\n",
    "- The right-hand term (regularization term) wants $w=0$\n",
    "\n",
    "In this particular case, because I picked such a simple example, we can do the math by hand. Taking the derivative and setting it to zero yields $\\frac{df}{dw}=2(w-1)+2\\lambda w=0$, so the minimum occurs at $w=\\frac{1}{1+\\lambda}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0,100,1000)\n",
    "plt.plot(x, 1/(1+x))\n",
    "plt.xlabel('$\\lambda$', fontsize=20)\n",
    "plt.ylabel('optimal $w$', fontsize=20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a _regularization path_ but in the very simple case of just one $w$. We recover the limiting cases:\n",
    "\n",
    "- $\\lambda=0$ gives us $w=1$\n",
    "- $\\lambda \\rightarrow \\infty$ gives us $w=0$\n",
    "\n",
    "All this pretty much generalizes to larger $n$ and $d$. So $\\lambda$ controls the relative importance of training error vs. regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regularization paths\n",
    "\n",
    "Here's an example with $d>1,n>1$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 10\n",
    "n = 30\n",
    "w_true = npr.randn(d)*3\n",
    "X = npr.randn(n,d)\n",
    "y = npr.randn(n)/5 + X@w_true\n",
    "λ_valsL2 = 10**np.linspace(-2,4,1000)\n",
    "λ_valsL1 = 10**np.linspace(-2,2,1000)\n",
    "\n",
    "WsL2 = np.zeros((len(λ_valsL2),d))\n",
    "WsL1 = np.zeros((len(λ_valsL1),d))\n",
    "for i,λ in enumerate(λ_valsL2):\n",
    "    ridge = Ridge(alpha=λ)\n",
    "    ridge.fit(X,y)\n",
    "    WsL2[i] = ridge.coef_\n",
    "for i,λ in enumerate(λ_valsL1):    \n",
    "    lasso = Lasso(alpha=λ)\n",
    "    lasso.fit(X,y)\n",
    "    WsL1[i] = lasso.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.semilogx(λ_valsL2, WsL2);\n",
    "plt.xlabel(\"λ\");\n",
    "plt.ylabel(\"$w_j$\");\n",
    "plt.title(\"L2 regularization\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.semilogx(λ_valsL1, WsL1);\n",
    "plt.xlabel(\"λ\");\n",
    "plt.ylabel(\"$w_j$\");\n",
    "plt.title(\"L1 regularization\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the weights \"snap\" to zero in the L1 case whereas they only approach 0 asymptotically in the L2 case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Bonus) Another question is what happens to regularization as $n$ gets large.\n",
    "\n",
    "- As $n$ gets large, the loss term starts to dominate the regularization term for fixed $\\lambda$, because it's a sum over $n$ things.\n",
    "  - This appeals to our intuition, since we don't expect to overfit if we have lots of training data.\n",
    "  - There's also a Bayesian interpretation coming later in the course.\n",
    "- In theory, $\\lambda$ should be $\\Omega(1)$ and $O(n^{1/2})$ (hopefully those are the correct symbols!). \n",
    "  - If it grew linearly with $n$ then the loss and regularization would grow the same amount, which violates the above intuition about lots of training data.\n",
    "- In practice, if we're choosing $\\lambda$ with (cross)-validation anyway, we'll just get the value that works best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why not use Huber regularization?\n",
    "\n",
    "- Don’t confuse the L1 loss with L1-regularization!!!\n",
    "  - L1-loss is robust to outlier data points.\n",
    "    - You can use instead of removing outliers.\n",
    "    - \"sparse residuals\"\n",
    "  - L1-regularization is robust to irrelevant features.\n",
    "    - You can use instead of removing features.\n",
    "    - \"sparse coefficients/weights\"\n",
    "- And note that you can use them together.\n",
    "- Why aren’t we smoothing and using \"Huber regularization\"?\n",
    "   - With the L1 loss, we cared about its behavior far from 0.\n",
    "   - With L1 regularization, we care about its behavior near 0.\n",
    "      - It’s precisely the non-smoothness that sets weights to exactly 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple choice question\n",
    "\n",
    "Which of the following is true about L1 regularization? (There may be multiple that are true.)\n",
    "\n",
    "1. For any value of $\\lambda$, some of the $w$-values will be $0$.\n",
    "2. For $\\lambda \\rightarrow \\infty$ all of the $w$-values will be zero.\n",
    "3. All the weights will be smaller than with L2 regularization, because you're not squaring them in the loss.\n",
    "4. The regularization path is **not** a continuous function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uniqueness of the solution: L1 and L2\n",
    "\n",
    "- With no regularization, the solution is not unique. \n",
    "  - Example: two identical columns $i$ and $j$. \n",
    "    - If $(w_i,w_j)$ is a solution then $(w_j,w_i)$ is also a solution.\n",
    "- With L1 regularization, the solution is also not unique. \n",
    "  - The same example applies.\n",
    "- With L2 regularization, the solution is unique. \n",
    "  - If we have two identical columns $i$ and $j$ then $w_i=w_j$.\n",
    "  \n",
    "**The experiments shown below bonus material.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npr.seed(0)\n",
    "\n",
    "n = 20\n",
    "X = npr.randn(n,1)\n",
    "y = npr.randn(n,1) + 0.5*X**2 + X\n",
    "X = np.concatenate((X,X),axis=1) # identical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridX = np.linspace(-3,3,1000)\n",
    "gridY = np.linspace(-3,3,1000)\n",
    "meshX, meshY = np.meshgrid(gridX, gridY)\n",
    "\n",
    "def make_plot(loss_fun, **kwargs):\n",
    "    loss = loss_fun(np.c_[meshX.ravel(), meshY.ravel()].T, X, y, **kwargs)\n",
    "    min_inds = np.nonzero(loss<=np.min(loss)*(1+1e-7))\n",
    "    loss = loss.reshape(meshX.shape[0],meshX.shape[1])\n",
    "    # plt.imshow(loss,extent=(np.min(gridX), np.max(gridX), np.min(gridY), np.max(gridY)), origin='lower')\n",
    "    CS = plt.contourf(gridX, gridY, loss, 30);\n",
    "    plt.plot(meshX.flatten()[min_inds], meshY.flatten()[min_inds], '.r');\n",
    "    plt.axis('square');\n",
    "    # plt.colorbar();\n",
    "    plt.xlabel('$w_1$');\n",
    "    plt.ylabel('$w_2$');\n",
    "make_plot(f)\n",
    "plt.title(\"No regularization\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a \"smear\" of solutions (red) along the $w_1+w_2=w_\\text{total}$ axis for some $w_\\text{total}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X,y)\n",
    "print(\"w_total = %.4f\"%np.sum(lr.coef_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_plot(f1, λ=1)\n",
    "plt.title(\"L1 regularization\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This is very interesting! We still have $w_1+w_2=w_\\text{total}$ but...\n",
    "- we can't have one of them take the opposite sign of $w_\\text{total}$ (in this case positive)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_plot(f2, λ=0.1)\n",
    "plt.title(\"L2 regularization\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With L2, the solution is unique. Why? This isn't a general proof but, try minimizing $w_1^2+w_2^2$ subject to $w_1+w_2=w_\\text{total}$. You'll get $w_1=w_2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
